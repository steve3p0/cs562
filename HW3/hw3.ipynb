{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: RNNs\n",
    "\n",
    "In this assignment, you will use PyTorch and RNNs to solve two NLP problems:\n",
    "\n",
    "1. Language identification\n",
    "2. Language modeling\n",
    "\n",
    "In the first, you will train a classifier to distinguish between English and Spanish sentences, and will essentially be creating a working version of the classifier demonstrated at the end of the PyTorch walkthrough from week 3's lecture. In the second you will train a neural language model to generate new British place names. For the second part, you will find Chapter 17 of the Goldberg text to be a useful reference.\n",
    "\n",
    "**Note:** The networks that you will be training in this assignment are relatively simple, and the amounts of data are relatively small. That said, _**they will take time to train**_. On my laptop, the various networks take on the order of ≈5 minutes to train. Make sure not to wait until the last minute to do this assignment, as you will need to do a fair bit of experimentation and tinkering, and you won't want to find yourself running out of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "\n",
    "In addition to Jupyter and `nose`, you will need to have `pytorch`, `numpy`, `pandas`, and Scikit-Learn installed for this assignment. As in HW2, most of your actual coding will take place in the `hw3_utils` module, and we will use unit tests to verify that things are working appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other important thing to do is to set the random number generation seed. This will ensure deterministic results: each time you re-run the notebook, you should get the same random numbers. This is vital for ensuring reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42);\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's check and see what versions of the libraries you've got installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions:\n",
      "PyTorch: 1.0.1\n",
      "Pandas: 0.24.1\n",
      "Numpy: 1.16.1\n",
      "matplotlib: 3.0.2\n",
      "nose: 1.3.7\n",
      "sklearn: 0.20.2\n"
     ]
    }
   ],
   "source": [
    "print(\"My library versions:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"nose: {nose.__version__}\")\n",
    "print(f\"sklearn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can run our tests as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocessing\n",
    "\n",
    "### Vocabulary construction\n",
    "\n",
    "As in the last part of HW2, we will be representing our input as numeric vectors. As such, we need a way to map tokens (characters, in the case of this assignment) to numeric indices. `hw3_utils.vocab` will contain functions to do this sort of transformation. The first such function will be `build_vocab`, and will take as input a corpus of sentences and as output produce mapping dictionaries. `vocab` includes special `BOS_SYM` and `EOS_SYM` symbols, to represent the beginning and end of string, respectively- make sure that the vocabularies that your implementation builds includes these characters.\n",
    "\n",
    "* **Deliverable 1.1:** Complete the `vocab.build_vocab()` function\n",
    "* **Test:** `nosetests tests/test_vocab.py:test_d1_1_char_vocab`\n",
    "\n",
    "_Hint_: Remember that you can split a string into its sequence of characters using the `list` function. For _this_ assignment, you may ignore issues of Unicode decomposition and normalization, but in \"real life\" you would absolutely need to worry about such things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3_utils import vocab\n",
    "reload(vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus = [\"This is a sentence.\", \"This is another sentence.\", \"Here is a third.\"]\n",
    "c2i, i2c = vocab.build_vocab(tiny_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"T\" == i2c[c2i[\"T\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!nosetests tests/test_vocab.py:test_d1_1_char_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence representation\n",
    "\n",
    "In addition to a character inventory, we'll need a way to convert a sentence into a list of indicies. Ordinarily, one would need to worry about how to handle missing vocabulary entries (i.e., OOV words or symbols); _for this assignment_, we are ignoring this problem.\n",
    "\n",
    "* **Deliverable 1.2:** Complete the `vocab.sentence_to_vector()` function\n",
    "* **Test:** `nosetests tests/test_vocab.py:test_d1_2_sentence_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 5, 3, 0, 8, 3, 0, 8, 12, 8, 0, 2, 9, 10, 2, 9, 1, 2, 14]\n"
     ]
    }
   ],
   "source": [
    "s = \"This is a sentence.\"\n",
    "s_vec = vocab.sentence_to_vector(s, c2i)\n",
    "print(s_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 's',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i2c[c] for c in s_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch wants its input as `torch.Tensor` objects, not as Python `list`s, so we'll need to write one more short function to do that conversion. We'll want our tensors to be of size $(1,n)$, where $n$ is the length of the sentence in characters.\n",
    "\n",
    "* **Deliverable 1.3:** Complete the `sentence_to_tensor()` function\n",
    "* **Test:** `nosetests tests/test_vocab.py:test_d1_3_sentence_tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  5,  3,  0,  8,  3,  0,  8, 12,  8,  0,  2,  9, 10,  2,  9,  1,  2,\n",
      "         14]])\n",
      "torch.Size([1, 19])\n"
     ]
    }
   ],
   "source": [
    "s = \"This is a sentence.\"\n",
    "s_tens = vocab.sentence_to_tensor(s, c2i)\n",
    "print(s_tens)\n",
    "print(s_tens.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label lookup table\n",
    "\n",
    "Just as we need to be able to represent characters as indices, we also need to be able to map _labels_ to indicies. The logic for this is very similar to what we did before for characters, but without the tokenization step.\n",
    "\n",
    "* **Deliverable 1.4:** Complete the `vocab.build_label_vocab()` function\n",
    "* **Test:** `nosetests tests/test_vocab.py:test_d1_4_label_vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"eng\",\"eng\",\"spa\",\"eng\",\"deu\"]\n",
    "l2i, i2l = vocab.build_label_vocab(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2i['spa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2l[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l2i[l] for l in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Language identification\n",
    "\n",
    "Language identification is often one of the first steps of an NLP pipeline, and is a foundational task in NLP. _[Automatic Language Identification in Texts: A Survey](https://arxiv.org/pdf/1804.08186.pdf)_, by Jauhiainen et al., gives an excellent overview of the task and its rich history in our field, and I would _strongly_ recommend taking a few minutes to read it before beginning this part of the assignment.\n",
    "\n",
    "Formally, we can craft language ID as a classification problem, given text sample $x$ and language $l \\in L$, we want to find: \n",
    "\n",
    "$$ \\operatorname*{argmax}_{l} P(l | x ) $$\n",
    "\n",
    "This can be done in a variety of ways. One common (and well-performing) technique is to use trigram character frequencies, or other character-level features (writing system, etc.); some approaches rely on or incorporate word-level features. The difficulty of the task depends on a number of factors:\n",
    "\n",
    "* How much training data one has from each language of interest (often an issue)\n",
    "* How similar the languages are to one another (Spanish and Italian are more difficult to differentiate than are English and Arabic)\n",
    "* How much [\"code-switching\"](https://en.wikipedia.org/wiki/Code-switching) is present in the language of interest\n",
    "\n",
    "The last point is a major one: many texts of interest (tweets, etc.) contain a mix of languages, which can cause trouble both at the time of _training_ a language identification system as well as at the time of _inference_ (i.e., when you're actually trying to _use_ your system). The problem can be compounded by mixed writing systems: due to limitations of text-entry systems, users of some non-Latin scripts often will often write e.g. Arabic using English orthography. Names of places, brands, individuals, etc. can also cause script-level code switching. Fortunately, for this assignment, you will not need to worry about this issue.\n",
    "\n",
    "We will be using [data](https://tatoeba.org/eng/downloads) from [Tatoeba](https://tatoeba.org), an online crowd-sourced language learning platform. Users collaboratively develop parallel corpora of sentences in a wide variety of languages. For this assignment, I have prepared two data sets based on Tatoeba's data:\n",
    "\n",
    "* [sentences_bilingual.csv](./data/sentences_bilingual.csv): 10,000 random English and 10,000 random Spanish sentences\n",
    "* [sentences_multilingual.csv](./data/sentences_multilingual.csv): 10,000 random sentences from each of English, Spanish, German, Italian, and French\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "We'll begin with the bilingual (english/spanish) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6733426</td>\n",
       "      <td>eng</td>\n",
       "      <td>Tom wasn't able to change anything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256835</td>\n",
       "      <td>eng</td>\n",
       "      <td>I saw the sights of Kyoto during my vacation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1404674</td>\n",
       "      <td>eng</td>\n",
       "      <td>And where are the aliens, the flying saucers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4833334</td>\n",
       "      <td>eng</td>\n",
       "      <td>Is the water in the Black Sea actually black?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7241028</td>\n",
       "      <td>eng</td>\n",
       "      <td>I know a lot of people who don't eat meat.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id lang                                           sentence\n",
       "0  6733426  eng                Tom wasn't able to change anything.\n",
       "1   256835  eng      I saw the sights of Kyoto during my vacation.\n",
       "2  1404674  eng  And where are the aliens, the flying saucers, ...\n",
       "3  4833334  eng      Is the water in the Black Sea actually black?\n",
       "4  7241028  eng         I know a lot of people who don't eat meat."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_text = pd.read_csv(\"data/sentences_bilingual.csv\")\n",
    "bi_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 is _always_ to look at your data. Verify how many entries we have, and of what languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 20000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total entries: {len(bi_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentence\n",
       "lang                 \n",
       "eng   10000     10000\n",
       "spa   10000     10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_text.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a character inventory/vocabulary for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2i, i2c = vocab.build_vocab(bi_text.sentence.values)\n",
    "l2i, i2l = vocab.build_label_vocab(bi_text.lang.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's make a 80/20 training-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "bi_text_train, bi_text_test = train_test_split(bi_text, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the resulting class distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>7981</td>\n",
       "      <td>7981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>8019</td>\n",
       "      <td>8019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentence\n",
       "lang                \n",
       "eng   7981      7981\n",
       "spa   8019      8019"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_text_train.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentence\n",
       "lang                \n",
       "eng   2019      2019\n",
       "spa   1981      1981"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_text_test.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's close enough to equal to be usable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the data set is _balanced_ between two categories, English and Spanish.\n",
    "\n",
    "The classification model you will build will first project sparse character representations to a dense learned embedding representation (as in Goldberg 4.8), feed the result into an LSTM, and then use the LSTM's output as the input into a softmax classifier to predict the language of the input. Note that we want to use the _final_ LSTM output for classification. More formally:\n",
    "\n",
    "$$ \\hat{y} = \\operatorname*{softmax} \\mathbf{o}^{(n)} $$\n",
    "\n",
    "$$ \\mathbf{o} = \\mathbf{h} \\mathbf{W}_o^T + b_o $$\n",
    "\n",
    "$$ \\mathbf{h} = LSTM(\\mathbf{x})$$\n",
    "\n",
    "$$ \\mathbf{x}_{1:n} = \\mathbf{E}[w_1], ... , \\mathbf{E}[w_n] $$\n",
    "\n",
    "Where $\\mathbf{E}$ is an embedding layer, with its own learned weights, etc. We can use PyTorch's [`nn.Embedding`](https://pytorch.org/docs/stable/nn.html#embedding) layer class to take care of this part of our network- it takes as input a character index (just like the ones produced by our vocabulary function above). Similarly, we can use [`nn.LSTM`](https://pytorch.org/docs/stable/nn.html#lstm), [`nn.Linear`](https://pytorch.org/docs/stable/nn.html#linear), and [`nn.LogSoftmax`](https://pytorch.org/docs/stable/nn.html#logsoftmax) to create the rest of the layers.\n",
    "\n",
    "This architecture has several hyperparameters that you must specify:\n",
    "\n",
    "0. How many distinct characters do you need to represent ($n$)?\n",
    "1. How many dimensions to use for embedding (representing) the characters ($e$)?\n",
    "2. How many dimensions (hidden units) should the LSTM have ($h$)?\n",
    "3. How many layers should the LSTM have?\n",
    "3. How many output classes should the network have ($c$)?\n",
    "\n",
    "The dimensionality of the layers of the network should then be: $n \\times e$ for the embedding layer, $e \\times h$ for the LSTM, and $h \\times c$ for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assignment, I have done much of the work for you, and have provided a skeleton class with completed initialization functions. \n",
    "\n",
    "* **Deliverable 2.1:** Following the MLP example in the PyTorch walkthrough from Week 3, complete the `forward()` method in the `nn.Module` that is started for you in `hw3_utils.lang_id`. \n",
    "* **Test:** `nosetests tests/test_lang_id.py:test_d2_1_forward`\n",
    "\n",
    "\n",
    "\n",
    "_Note_: typically, PyTorch networks are trained using minibatch training (see Goldberg 5.2.8); for the sake of keeping this assignment simple, we will not be using the standard PyTorch batching behaviors. Instead, we will be batching \"manually\"- calling `.backward()` and updating our parameters after a configurable interval. This will make training take a little bit longer, but will make _your_ code simpler. As such, the shape of the input will be slightly different than PyTorch expects by default. You may need to experiment a bit!\n",
    "\n",
    "Also note that this is a \"many-to-one\" RNN architecture. Out of the box, `nn.LSTM` is set up for _many-to-many_ architectures where the input and output are synchronized- in other words, if you feed five time points _in_ to the LSTM (i.e., providing a $(1,5)$ input), you'll get a $(1,5)$ _output_ back. Since we only care about the _last_ timepoint's output for our classification problem, at some point you'll need to extract that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3_utils import lang_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am initializing the model with 10 embedding dimensions, 20 hidden dimensions, a single LSTM layer, and two output classes (_`eng`_ and _`spa`_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lang_id);\n",
    "\n",
    "li = lang_id.LangID(\n",
    "    input_vocab_n=len(c2i), \n",
    "    embedding_dims=10,\n",
    "    hidden_dims=20,\n",
    "    lstm_layers=1,\n",
    "    output_class_n=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's initialized, I can provide it with input and verify that the output is of the dimensionality I expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8121, -0.5869], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li(vocab.sentence_to_tensor(\"this is a sentence\", c2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li(vocab.sentence_to_tensor(\"this is a sentence\", c2i)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: I recommend building up the `forward()` method one layer at a time, at each step of the way verifying that the output you are seeing is what you expect.\n",
    "\n",
    "That output is the raw log-probabilities associated with each class. This is not especially user-friendly, and makes evaluation a bit more difficult than we might care for. \n",
    "\n",
    "* **Deliverable 2.2:** Implement the `predict_one()` function in `hw3_utils.lang_id`.\n",
    "* **Test:** `nosetests tests/test_lang_id.py:test_d2_2_predict_one`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lang_id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spa'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_id.predict_one(li, \"this is a sentence\", c2i, i2l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating model\n",
    "\n",
    "Next, we need to train and evaluate the model. I have provided a `train_model()` function to do the training for this model, in `hw3_utils.lang_id`. This part of the assignment features fewer automated tests, and involves a little bit more thinking and writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lang_id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/16000 average per-item loss: 0.005609301850199699\n"
     ]
    }
   ],
   "source": [
    "trained_model = lang_id.train_model(\n",
    "    model=li, \n",
    "    n_epochs=1,\n",
    "    training_data=bi_text_train,\n",
    "    c2i=c2i, i2c=i2c,\n",
    "    l2i=l2i, i2l=i2l\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model(vocab.sentence_to_tensor(\"this is a sentence\", c2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_model(vocab.sentence_to_tensor(\"quien estas\", c2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a trained model, it's time to evaluate its performance. \n",
    "\n",
    "* **Deliverable 2.3:** Complete the `eval_acc()` function in `hw3_utils.lang_id`. Note that PyTorch `Tensor` objects have an `argmax()` function!\n",
    "* **Test:** `nosetests tests/test_lang_id.py:test_d2_3_eval_acc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, y_hat = lang_id.eval_acc(trained_model, bi_text_test, c2i, i2c, l2i, i2l)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what if we run this on a new, _untrained_ model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained = lang_id.LangID(\n",
    "    input_vocab_n=len(c2i), \n",
    "    embedding_dims=10,\n",
    "    hidden_dims=20,\n",
    "    lstm_layers=1,\n",
    "    output_class_n=2\n",
    ")\n",
    "\n",
    "acc_untrained, y_hat_untrained = lang_id.eval_acc(untrained, bi_text_test, c2i, i2c, l2i, i2l)\n",
    "\n",
    "print(f\"Accuracy: {acc_untrained}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, an untrained model gets approximately chance performance.\n",
    "\n",
    "On our trained model, we were able to achieve ≈90% accuracy on held-out test data, which is very high accuracy! Let's investigate our hyperparameter settings: how small can we make our embedding and hidden dimensions before performance suffers?\n",
    "\n",
    "* **Deliverable 2.4:** Turn both the embedding dimensions and the hidden dimensions _waaaaay_ down, to 2 each, and retrain/re-evaluate the model. Save your evaluation results to a CSV file named \"deliverable_2.4.csv\" with three columns: the sentence itself, the _true_ label, and the _predicted_ label. Also produce a text file named \"deliverable_2.4.txt\" containing the observed evaluation accuracy and an explanation of what you observe. Why is the number what it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending to multinomial classification\n",
    "\n",
    "Clearly, a simple LSTM classifier works very well for this binary classification problem, even for very low-dimensional networks. What if we try and do multi-class classification? Our model should, in theory, be adaptable to that case. How sensitive is it to our network dimensionality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deu</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fra</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentence\n",
       "lang                 \n",
       "deu   10000     10000\n",
       "eng   10000     10000\n",
       "fra   10000     10000\n",
       "ita   10000     10000\n",
       "spa   10000     10000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_text = pd.read_csv(\"data/sentences_multilingual.csv\")\n",
    "multi_text_train, multi_text_test = train_test_split(multi_text, test_size=0.2)\n",
    "\n",
    "multi_text.groupby('lang').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deu</th>\n",
       "      <td>7994</td>\n",
       "      <td>7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>8024</td>\n",
       "      <td>8024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fra</th>\n",
       "      <td>7949</td>\n",
       "      <td>7949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita</th>\n",
       "      <td>8012</td>\n",
       "      <td>8012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>8021</td>\n",
       "      <td>8021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentence\n",
       "lang                \n",
       "deu   7994      7994\n",
       "eng   8024      8024\n",
       "fra   7949      7949\n",
       "ita   8012      8012\n",
       "spa   8021      8021"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_text_train.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2i, i2c = vocab.build_vocab(multi_text.sentence.values)\n",
    "l2i, i2l = vocab.build_label_vocab(multi_text.lang.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/40000 average per-item loss: 0.014599330723285675\n",
      "1000/40000 average per-item loss: 1.5959402322769165\n",
      "2000/40000 average per-item loss: 1.6023319959640503\n",
      "3000/40000 average per-item loss: 1.6038333177566528\n",
      "4000/40000 average per-item loss: 1.590908408164978\n",
      "5000/40000 average per-item loss: 1.5808950662612915\n",
      "6000/40000 average per-item loss: 1.5569266080856323\n",
      "7000/40000 average per-item loss: 1.5691877603530884\n",
      "8000/40000 average per-item loss: 1.5350695848464966\n",
      "9000/40000 average per-item loss: 1.5278592109680176\n",
      "10000/40000 average per-item loss: 1.4628705978393555\n",
      "11000/40000 average per-item loss: 1.456663966178894\n",
      "12000/40000 average per-item loss: 1.3107162714004517\n",
      "13000/40000 average per-item loss: 1.3391351699829102\n",
      "14000/40000 average per-item loss: 1.2879527807235718\n",
      "15000/40000 average per-item loss: 1.2821139097213745\n",
      "16000/40000 average per-item loss: 1.2367126941680908\n",
      "17000/40000 average per-item loss: 1.1812931299209595\n",
      "18000/40000 average per-item loss: 1.2160165309906006\n",
      "19000/40000 average per-item loss: 1.282893419265747\n",
      "20000/40000 average per-item loss: 1.1069241762161255\n",
      "21000/40000 average per-item loss: 1.171472191810608\n",
      "22000/40000 average per-item loss: 1.0402315855026245\n",
      "23000/40000 average per-item loss: 1.0539418458938599\n",
      "24000/40000 average per-item loss: 1.053621768951416\n",
      "25000/40000 average per-item loss: 1.084457278251648\n",
      "26000/40000 average per-item loss: 1.0078010559082031\n",
      "27000/40000 average per-item loss: 0.951377272605896\n",
      "28000/40000 average per-item loss: 0.9940849542617798\n",
      "29000/40000 average per-item loss: 0.9421520829200745\n",
      "30000/40000 average per-item loss: 0.9632700085639954\n",
      "31000/40000 average per-item loss: 0.9325721263885498\n",
      "32000/40000 average per-item loss: 0.787610650062561\n",
      "33000/40000 average per-item loss: 0.8943670392036438\n",
      "34000/40000 average per-item loss: 0.8686915040016174\n",
      "35000/40000 average per-item loss: 0.9151987433433533\n",
      "36000/40000 average per-item loss: 0.8247097730636597\n",
      "37000/40000 average per-item loss: 0.8836609721183777\n",
      "38000/40000 average per-item loss: 0.7788248658180237\n",
      "39000/40000 average per-item loss: 0.691580593585968\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "multi_class = lang_id.LangID(\n",
    "    input_vocab_n=len(c2i), \n",
    "    embedding_dims=10,\n",
    "    hidden_dims=20,\n",
    "    lstm_layers=1,\n",
    "    output_class_n=5\n",
    ")\n",
    "\n",
    "lang_id.train_model(\n",
    "    model=multi_class, \n",
    "    n_epochs=1,\n",
    "    training_data=multi_text_train,\n",
    "    c2i=c2i, i2c=i2c,\n",
    "    l2i=l2i, i2l=i2l\n",
    ");\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's trained, let's evaluate it as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6954\n"
     ]
    }
   ],
   "source": [
    "acc_multi, y_hat_multi = lang_id.eval_acc(multi_class, multi_text_test, c2i, i2c, l2i, i2l)\n",
    "\n",
    "print(f\"Accuracy: {acc_multi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 5-way classification, a single accuracy number is less useful than it is for binary classification. Scikit-Learn has some helpful classes for evaluating classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classification_report` function will give us a per-class breakdown of our performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         deu       0.82      0.75      0.79      2006\n",
      "         eng       0.75      0.75      0.75      1976\n",
      "         fra       0.69      0.78      0.73      2051\n",
      "         ita       0.61      0.66      0.63      1988\n",
      "         spa       0.61      0.53      0.57      1979\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     10000\n",
      "   macro avg       0.70      0.69      0.69     10000\n",
      "weighted avg       0.70      0.70      0.69     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_multi = multi_text_test.lang.values\n",
    "print(classification_report(y_multi, y_hat_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `confusion_matrix` function will give us a raw confusion matrix; rows are \"truth\" and columns are \"predictions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1512,  334,  124,   31,    5],\n",
       "       [ 272, 1479,  116,   76,   33],\n",
       "       [  32,   83, 1592,  160,  184],\n",
       "       [  14,   48,  166, 1313,  447],\n",
       "       [   5,   40,  298,  578, 1058]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_multi, y_hat_multi)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lang_id` module has a method to make nicer-looking matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lang_id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang_id.pretty_conf_matrix(cm, ['deu','eng','fra','ita','spa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Deliverable 2.5:** Interpret these results. Why is our classifier performing better on some languages than on others? Put your analysis in a file named `deliverable_2.5.txt`.\n",
    "\n",
    "From here, the rest of this section is up to you. Experiment with different numbers of embedding & hidden dimensions, hidden LSTM layers, etc., and see how high you can get your classification accuracy. Also try training for additional epochs. Does that help?\n",
    "\n",
    "* **Deliverable 2.6:** Describe at least two additional models that you trained and evaluated, along with their overall and per-class accuracies. What seemed to help the most? Put your discussion in a file named `deliverable_2.6.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Language modeling\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "In this part of the assignment, you will build and evaluate a simple LSTM character language model, and then _sample_ from it to generate new text mimicking the training data. The model you will build should follow the general structure outlined in Section 17.1 of the Goldberg text, and use the \"teacher-forcing\" training method outlined in 17.1.1.\n",
    "\n",
    "$$ p(\\mathbf{w}_n | \\mathbf{w}_{1:n-1}) = \\operatorname*{softmax} \\mathbf{o}^{(n-1)} $$\n",
    "\n",
    "$$ o = \\mathbf{h} \\mathbf{W}_o^T + b_o $$\n",
    "\n",
    "$$ \\mathbf{h} = LSTM(\\mathbf{x})$$\n",
    "\n",
    "$$ \\mathbf{x}_{1:n-1} = \\mathbf{E}[w_1], ... , \\mathbf{E}[w_n-1] $$\n",
    "\n",
    "For this part of the assignment, we will be working with a data set of British place names located in `data/towns_clean.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [l.strip() for l in open('data/towns_clean.txt')]\n",
    "c2i, i2c = vocab.build_vocab(corpus)\n",
    "\n",
    "for t in np.random.choice(corpus, size=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your network, once trained, will learn to recognize British place names, and will be able to make up new ones.\n",
    "\n",
    "* **Deliverable 3.1:** Implement the `__init__()`, and `forward()` functions in `hw3_utils.lm.NameGenerator`.\n",
    "* **Test:** `nosetests tests/test_lm.py:test_d3_1_setup`\n",
    "\n",
    "Once you've got these methods set up correctly, the following ought to work (in the sense of producing appropriately-sized output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3_utils import lm\n",
    "reload(lm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lm.NameGenerator(\n",
    "    input_vocab_size=len(c2i),\n",
    "    n_embedding_dims=25,\n",
    "    n_hidden_dims=50,\n",
    "    n_lstm_layers=1,\n",
    "    output_vocab_size=len(c2i)\n",
    ")\n",
    "\n",
    "x = vocab.sentence_to_tensor(\"Test input\", c2i, True) # make sure to pad w/ bos/eos symbols\n",
    "y_hat, lstm_hidden_state = model(x, model.init_hidden())\n",
    "y_hat.shape # should be 1x12x77 (1 x len(\"Test input\")+2 x len(c2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each position $i$ along the second dimension of $\\hat{y}$ is the model's estimate of $p(x_{i+1} | x_{1:i})$, and is represented as a multinomial probability distribution in log-space (i.e., each of the 78 possible characters is assigned an estimated log-probability of occurring).\n",
    "\n",
    "By multiplying across (or summing across, in log-space) each of those values, we can calculate the model's estimate of the probability of an input sequence (see Jurafsky & Martin, Chapter 3 if you need refresher on how this works). As a convenience method, I have provided `lm.compute_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm.compute_prob(model, \"Test input\", c2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.compute_prob(model, \"fdsafsadfa\", c2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces _negative log probabilities_. In this case, the model has not yet been trained and so the two sequences are given (very roughly) equivalent probabilities\n",
    "\n",
    "Note that \"out of the box\" `compute_prob()` does not _normalize_ the probabilities by sequence length, and as such longer sequences will have lower probabilities on average than shorter sequences, regardless of how plausible they are to the model. If you want to compare the model's estimate of the probabilities of two sequences of different lengths, you must normalize each by the appropriate sequence's length.\n",
    "\n",
    "Once the model is trained, the estimated probabilities of the two sequences above should be quite different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Note that there are two ways to structure the training of this sort of model.\n",
    "\n",
    "1. One character at a time\n",
    "2. In a quasi batched manner\n",
    "\n",
    "The equations above describe character-at-a-time training- starting from the shortest possible prefix of our sequence $\\mathbf{w}$, $\\mathbf{w}_{1:1}$, and continuing up through successively longer prefixes until reaching $\\mathbf{w}_{1:n-1}$, ask the model to predict the next character, accumulating loss with the negative log-likelihood criterion.\n",
    "\n",
    "In this scenario, if our input training sentence was _\"hello there\"_, we would perform a series of forward passes:\n",
    "\n",
    "$x_1=\"h\"$, $y_1=\"e\"$\n",
    "\n",
    "$x_2=\"he\"$, $y_2=\"l\"$\n",
    "\n",
    "$x_3=\"hel\"$, $y_3=\"l\"$\n",
    "\n",
    "... and so forth, either doing a call to `backward()` after each prefix $x_i$ or waiting until the end of the sequence to do so. \n",
    "\n",
    "This way works just fine, and is more or less how we will go about sampling from the model, but it is often more efficient to do a more batched training operation. Note that in the above equations, $\\mathbf{o}$ will have dimensionality of $(1, n-1, N_{classes})$- in other words, there will be one set of probability estimates for each time point that we have provided the LSTM. PyTorch's `nn.NLLLoss` class can accept input in this shape, and will automatically compute the average loss across the timepoints (and of course will do so a way that keeps the gradients around for correct backpropagation). Training this way can reduce the total number of computations that we need to do in order to train our model.\n",
    "\n",
    "In this scenario, we would perform a _single_ forward and backward pass, with input that looked like so:\n",
    "\n",
    "$x=\"hello\\_ther\"$, $y=\"ello\\_there\"$\n",
    "\n",
    "Note that PyTorch is perfectly happy either way: the various `nn.*` classes all expect to be able to be given input in this minibatched format. In the first training strategy, the \"batch\" size is 1; in the second, it is $n-1$ where $n$ is the length of the training example.\n",
    "\n",
    "The overall process of training this model will be very similar to how we trained the language ID classifier, so use that as a starting point and modify as needed. For starters, train with the following parameters:\n",
    "\n",
    "* Number of character embedding dimensions: 25\n",
    "* Number of LSTM hidden units: 50\n",
    "* Number of LSTM layers: 1\n",
    "\n",
    "Once your model is trained, you can save it to disk using the `torch.save()` function. This will make it available for the unit test to access.\n",
    "\n",
    "* **Deliverable 3.2:** Implement the `train()` function in `hw3_utils.lm`. Save a trained model to a file named `deliverable_3.2.mod`.\n",
    "* **Test:** `nosetests tests/test_lm.py:test_d3_2_training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.train(model, epochs=2, training_data=corpus, c2i=c2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm.compute_prob(model, \"Test input\", c2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.compute_prob(model, \"fdsafsadfa\", c2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, this is in log-space, so the difference here is substantial. And now let's try giving it an actual town name as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.compute_prob(model, \"Wavertree\", c2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "Now that we've got a language model trained, we can sample from it to generate _new_ names. Follow the general procedure outlined in Goldberg 9.5: \n",
    "\n",
    "1. Build an initial history consisting of the `vocab.BOS_SYM` symbol\n",
    "2. Using this history, query the language model, and predict a distribution over next characters\n",
    "3. Sample a character from that distribution\n",
    "4. Concatenate the selected character to make a _new_ history\n",
    "5. Repeat from step 2 until the selected character is `vocab.EOS_SYM` or we hit a maximum length (200 characters, in this assignment).\n",
    "\n",
    "Once you've implemented sampling, you can generate hilarious and adorable new names!\n",
    "\n",
    "* **Deliverable 3.3:** Implement the `sample()` function in `hw3_utils.lm`\n",
    "* **Test:** `nosetests tests/test_lm.py:test_d3_3_sample`\n",
    "\n",
    "_Hint_: The `torch.multinomial()` function samples from a multinomial distribution stored in a `Tensor`, but _it requires real-space probabilities_, not log-space. `Tensor`s have a `.exp()` method that you may need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(lm.sample(model, c2i, i2c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving your model\n",
    "\n",
    "These generated names should look more like names of towns than random sequences, but they are a little bit rough compared to the names in the data file. Try modifying your training process to go for additional epochs, and see how that changes your sampled sentences. Also, experiment with different model parameters. What happens if you _decrease_ number of hidden dimensions? What about _increasing_ the dimensionality? How about adding more layers to the LSTM?\n",
    "\n",
    "One useful thing to try: now that the `sample()` function is written, you might try adding some reporting code to your `train()` function: at regular intervals during training (e.g. every 1000 training examples), you could have it print out a few random towns generated from the model at its current state. This way, you can watch the model evolve over time. How many examples does it need to see before it learns that names always start with upper-case letters?\n",
    "\n",
    "* **Deliverable 3.4:** Write up a short summary of your observations and investigations into model behavior, including examples, and put it in a text file called `deliverable_3.4.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
