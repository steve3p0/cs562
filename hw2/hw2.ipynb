{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Text Classification\n",
    "\n",
    "Text classification is a fundamental NLP task. In this assignment, you will build and evaluate a simple text classification pipeline, using both from-scratch as well as library implementations of standard classification algorithms.\n",
    "\n",
    "This problem set is adapted from the first problem set given in Jacob Eisentstein's Spring Term 2018 course in NLP, taught at Georgia Tech, and is modified from [the original](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/psets/ps1/pset1.ipynb) with permission of the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "You will need to install several Python libraries in order to do this assignment. They are all installable with either `anaconda` or `pip`.\n",
    "\n",
    "* jupyter\n",
    "* matplotlib\n",
    "* sklearn\n",
    "* nose\n",
    "* pandas\n",
    "\n",
    "Most of your coding for this assignment will be in files in the `hw2_utils` package as well as in this Jupyter notebook. I have also provided automated unit tests which will help you determine if your code is working correctly, located in the `tests` directory. You are free to examine the code for the unit tests- in fact, you _should_ examine the code, as you will likely find it helpful. Note that these tests are the same ones that I will be using to help grade the assignment, so one of your goals with this assignment is to get all of the tests to pass. For more information about working with unit tests, see [this guide](http://pythontesting.net/framework/nose/nose-introduction/).\n",
    "\n",
    "Note that the skeleton function implementations in `hw2_utils` all have careful and precise docstring annotations indicating types of parameters and returns. Pay careful attention to these, as you will be using your own functions elsewhere in the assignment!\n",
    "\n",
    "If you have never used Jupyter notebooks before, [this guide](https://www.dataquest.io/blog/jupyter-notebook-tutorial/) may be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification & loading of packages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not already have the relevant packages installed, execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: nose in /home/steve/venv/lib/python3.6/site-packages (1.3.7)\n",
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 4.6MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/steve/venv/lib/python3.6/site-packages (0.24.0)\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 8.8MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.0 in /home/steve/venv/lib/python3.6/site-packages (from matplotlib) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/steve/venv/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/0a/001be530836743d8be6c2d85069f46fecf84ac6c18c7f5fb8125ee11d854/pyparsing-2.3.1-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 31.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
      "\u001b[K    100% |████████████████████████████████| 952kB 23.1MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2011k in /home/steve/venv/lib/python3.6/site-packages (from pandas) (2018.9)\n",
      "Collecting scipy>=0.13.3 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 26.6MB 2.5MB/s eta 0:00:011    82% |██████████████████████████▌     | 22.0MB 35.3MB/s eta 0:00:01    96% |███████████████████████████████ | 25.7MB 35.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/steve/venv/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/steve/venv/lib/python3.6/site-packages/setuptools-39.1.0-py3.6.egg (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n",
      "\u001b[31mjupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.15 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scipy, scikit-learn, sklearn, pyparsing, cycler, kiwisolver, matplotlib\n",
      "  Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed cycler-0.10.0 kiwisolver-1.0.1 matplotlib-3.0.2 pyparsing-2.3.1 scikit-learn-0.20.2 scipy-1.2.0 sklearn-0.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sklearn nose matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What versions do you have installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 (default, Oct 22 2018, 11:32:17) \n",
      "[GCC 8.2.0]\n",
      "My library versions:\n",
      "Pandas: 0.24.0\n",
      "Numpy: 1.16.0\n",
      "Scipy: 1.2.0\n",
      "matplotlib: 3.0.2\n",
      "nose: 1.3.7\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(\"My library versions:\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"Scipy: {sp.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"nose: {nose.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our first unit test, to make sure that your environment is set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "We'll be using Pandas dataframes to do some of our work. Start by reading a chunk of the data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('lyrics-train.csv')\n",
    "df_dev = pd.read_csv('lyrics-dev.csv')\n",
    "df_test = pd.read_csv('lyrics-test-hidden.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Era</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre-1980</td>\n",
       "      <td>come on come on let me show you where its at a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980s</td>\n",
       "      <td>welcome to the big time youre bound to be a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pre-1980</td>\n",
       "      <td>once i believed that when love came to me it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000s</td>\n",
       "      <td>i took my love and i took it down climbed a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pre-1980</td>\n",
       "      <td>do do do do do do do do do do do do do do do ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Era                                             Lyrics\n",
       "0  pre-1980  come on come on let me show you where its at a...\n",
       "1     1980s   welcome to the big time youre bound to be a s...\n",
       "2  pre-1980   once i believed that when love came to me it ...\n",
       "3     2000s   i took my love and i took it down climbed a m...\n",
       "4  pre-1980   do do do do do do do do do do do do do do do ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "The first step is to write functions to produce a bag-of-words representation of each input song (using counts). For this data set, much of the work is already done: each song is already lower-cased, tokenized appropriately (modulo a few errors), and has had its punctuation removed.\n",
    "\n",
    "* **Deliverable 1.1**: Complete the function `hw2_utils.preproc.bag_of_words`\n",
    "\n",
    "* **Test**: `nosetests tests/test_preproc.py:test_d1_1_bow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2_utils import preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block as you update the Python file, to trigger a reload in the notebook\n",
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train = preproc.read_data('lyrics-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 38.012s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_preproc.py:test_d1_1_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev, x_dev = preproc.read_data('lyrics-dev.csv')\n",
    "y_test, x_test = preproc.read_data('lyrics-test-hidden.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen words\n",
    "\n",
    "One challenge for text classification is that words may appear in the test data that do not appear in the training data. In order to know how big of a problem this will be, it is useful to do some exploratory data analysis into word frequencies and vocabulary overlap. Since we don't want to look at the test data itself, we'll start by looking into the number of words that appear in `lyrics-dev.csv` but not in `lyrics-train.csv`.\n",
    "\n",
    "* **Deliverable 1.2**: Implement `hw2_utils.preproc.aggregate_counts`, a function to combine [`Counter`s](https://docs.python.org/3/library/collections.html#collections.Counter) representing document-level bags-of-words into a single `Counter` representing a collection of documents.\n",
    "* **Deliverable 1.3**: Implement `hw2_utils.preproc.compute_oov`, which returns a list of words that appear in one bag-of-words but not another.\n",
    "* **Tests**: `tests/test_preproc.py:test_d1_2_agg`, `tests/test_preproc.py:test_d1_3_oov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tip_: You may find it worthwhile or useuful to use the `%%timeit` Jupyter magic directive to evaluate the performance of your algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 901 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "preproc.aggregate_counts(x_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Counter`s are very useful tools! Note that many common set operations (intersection, etc.) are available, but be warned that they are sometimes _not_ the most performant way to work with Counters. Consult the documentation for details. \n",
    "\n",
    "You can see the most items in a `Counter` using the `most_common()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 5542), ('i', 5535), ('the', 5061), ('to', 3203), ('and', 2953)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_dev = preproc.aggregate_counts(x_dev)\n",
    "counts_train = preproc.aggregate_counts(x_train)\n",
    "counts_dev.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 35.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "preproc.aggregate_counts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2677"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc.compute_oov(counts_dev, counts_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30428"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc.compute_oov(counts_train, counts_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.297246280257606"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.oov_rate(counts_dev, counts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like just under 30% of the words in the dev set do not appear in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power laws\n",
    "\n",
    "As we've discussed, word count distributions typically follow a [power law](https://en.wikipedia.org/wiki/Power_law) distribution.\n",
    "\n",
    "In practice, this means that a log-log plot of frequency against rank should be linear-ish. Let's see if this holds for our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcf1eb4fe48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog([val for word, val in counts_train.most_common()])\n",
    "plt.loglog([val for word, val in counts_dev.most_common()])\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend(['Training', 'Dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making plots like this is a very helpful \"sanity check\", to make sure that our data looks like we're expecting it to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the vocabulary\n",
    "\n",
    "Let's prune the vocabulary to only include words that appear at least _ten_ times in the training data.\n",
    "\n",
    "* **Deliverable 1.4**: Implement `preproc.prune_vocabulary`\n",
    "* **Test**: `tests/test_preproc.py:test_d1_4_prune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pruned, vocab = preproc.prune_vocabulary(counts_train, x_train, 10)\n",
    "x_dev_pruned, _ = preproc.prune_vocabulary(counts_train, x_dev, 10)\n",
    "x_test_pruned, _ = preproc.prune_vocabulary(counts_train, x_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig pruned\n",
      "88 0\n",
      "187 0\n"
     ]
    }
   ],
   "source": [
    "print(\"orig\",\"pruned\")\n",
    "print(len(x_dev[94]), len(x_dev_pruned[94]))\n",
    "print(sum(x_dev[94].values()), sum(x_dev_pruned[94].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear classification\n",
    "\n",
    "\n",
    "Now, you'll implement a simple linear classification rule, $\\hat{y}=\\text{argmax}_y \\theta^T f(x,y)$\n",
    "\n",
    "The functions you write in this section will be used for all the classifiers in the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2_utils import clf_base\n",
    "reload(clf_base);\n",
    "\n",
    "from hw2_utils import constants\n",
    "reload(constants);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to write a feature extraction function. A convenient way to represent a feature function vector $f(x,y)$ for a document $x$ is as a dict, where each value $j$ is the value of the $j$th feature $x_j$ (counts of word occurrence, or whatever it is that you're using as features), and the keys are tuples $(y, x_j)$ indicating _which_ feature-class relationship is being stored.\n",
    "\n",
    "* **Deliverable 2.1**: Implement the function `make_feature_vector` in `clf_base.py`\n",
    "* **Test**: `tests/test_classifier.py:test_d2_1_featvec`\n",
    "\n",
    "_Note_ that you'll also want to make sure to include the bias/offset feature as well, from `hw2_utils.constants.OFFSET`.\n",
    "\n",
    "Desired output is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1980s', '**OFFSET**'): 1, ('1980s', 'case'): 2, ('1980s', 'test'): 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fv = clf_base.make_feature_vector({'test':1, 'case':2}, '1980s')\n",
    "fv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the entire set of labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1980s', '1990s', '2000s', 'pre-1980'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(y_train) # all possible labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the prediction rule $\\hat{y}=\\text{argmax}_y \\theta^T f(x,y)$.\n",
    "\n",
    "* **Deliverable 2.2**: Implement the function `predict` in `clf_base.py`.\n",
    "* **Test**: `tests/test_classifier.py:test_d2_2_predict`\n",
    "\n",
    "The output should be:\n",
    "\n",
    "* A predicted label\n",
    "* The scores of all labels\n",
    "\n",
    "This function will be called a **lot**, so do try and make it fast. You don't need to do anything too over-the-top, but look for ways to avoid making your code do extra or repeated work. Remember, `%%timeit` can help you explore how different versions of your algorithm perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function using the unit test, but also with these simple hand-crafted weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight vectors _must_ be defaultdicts as per the specification for predict()\n",
    "theta_hand = defaultdict(float, \n",
    "            {('2000s', 'money'): 0.1,\n",
    "             ('2000s', 'name'): 0.2,\n",
    "             ('1980s','tonight'): 0.1,\n",
    "             ('2000s','man'): 0.1,\n",
    "             ('1990s','fly'): 0.1,\n",
    "             ('pre-1980',constants.OFFSET): 0.1\n",
    "\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(clf_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Counter' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e654b5e15485>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_pruned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_hand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\workspace_courses\\cs562\\hw2\\hw2_utils\\clf_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(base_features, weights, labels)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mbf_agg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;31m#bf_agg = sum(base_features, Counter())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\workspace_courses\\cs562\\hw2\\hw2_utils\\preproc.py\u001b[0m in \u001b[0;36maggregate_counts\u001b[1;34m(bags_of_words)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \"\"\"\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbags_of_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# deliverable 1.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Counter' and 'str'"
     ]
    }
   ],
   "source": [
    "clf_base.predict(x_train_pruned[0], theta_hand, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how well these weights work, by evaluating on the dev set. We'll be using raw classification accuracy here, though that is often not the best choice of metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2_utils import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_base.predict_all(x_dev_pruned, theta_hand, labels)\n",
    "print(evaluation.acc(y_hat, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just how imbalanced _are_ our classes, anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.unique(y_dev, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our `dev` data split, there is certainly some class imbalance, though it is not nearly as severe as is often the case in text classification. Using straight-up classification accuracy is not _totally_ unreasonable in this case, though one should make sure to compare it against \"random choice\" as well as \"most-frequent class\" baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naïve Bayes\n",
    "\n",
    "You'll now implement a Naïve Bayes classifier, as described in the reading. For a more detailed implementation guide, consult chapter 2 of the Eisenstein notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0b4c46f768a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhw2_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'reload' is not defined"
     ]
    }
   ],
   "source": [
    "from hw2_utils import naive_bayes\n",
    "reload(naive_bayes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Deliverable 3.1**: (warmup) implement `get_corpus_counts` in `naive_bayes.py`.\n",
    "* **Test**: `tests/test_classifier.py:test_d3_1_corpus_counts`\n",
    "\n",
    "This function should compute unigram word frequency counts for a given label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "eighties_counts = naive_bayes.get_corpus_counts(x_train_pruned, y_train, '1980s');\n",
    "print(eighties_counts['today'])\n",
    "print(eighties_counts['yesterday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Deliverable 3.2**: Implement `estimate_pxy` in `naive_bayes.py`\n",
    "* **Test**: `tests/test_classifier.py:test_d3_2_pxy`\n",
    "\n",
    "This function should compute the _smoothed_ multinomial distribution $\\log P(x|y)$ for a given label $y$.\n",
    "\n",
    "_Hint_: Note that this function takes the entire vocabulary as one of its arguments. You'll need to assign a probability even for words that do not appear in documents with label $y$, if they are in-vocabulary.\n",
    "\n",
    "You can use `get_corpus_counts` in this function if you want to, but you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pxy = naive_bayes.estimate_pxy(x_train_pruned, y_train, \"1980s\", 0.1, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your probabilities must sum to one! (Or very close- [floating point roundoff](https://en.wikipedia.org/wiki/Round-off_error#Roundoff_error_caused_by_floating-point_arithmetic), dontcha know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999512"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(list(log_pxy.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Emily Bender's wisdom, let's Look 👏 At 👏 Our 👏 Data!\n",
    "\n",
    "We can examine the log-probabilities of the words from our hand-tuned weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money': -7.689562807416732,\n",
       " 'name': -7.568324713816848,\n",
       " 'tonight': -6.216637557007502,\n",
       " 'man': -6.63187694645784,\n",
       " 'fly': -8.636944126360918,\n",
       " '**OFFSET**': 0.0}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word:log_pxy[word] for (_, word), weight in theta_hand.items() if weight > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we adjust our smoothing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pxy_smoother = naive_bayes.estimate_pxy(x_train_pruned, y_train, \"1980s\",10, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'money': -7.80136351255418,\n",
       " 'name': -7.6911603724205655,\n",
       " 'tonight': -6.405407240522552,\n",
       " 'man': -6.808471387093179,\n",
       " 'fly': -8.607451104294721,\n",
       " '**OFFSET**': 0.0}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word:log_pxy_smoother[word] for (_, word), weight in theta_hand.items() if weight > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all relatively frequent words, and so their smoothed estimates aren't really all that different. What if we look at a less-common word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "-11.481299646970376\n",
      "-9.951185850995815\n"
     ]
    }
   ],
   "source": [
    "print(eighties_counts['smiles'])\n",
    "print(log_pxy['smiles'])\n",
    "print(log_pxy_smoother['smiles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much larger change- remember, we're in log-space!\n",
    "\n",
    "You are now ready to...\n",
    "\n",
    "* **Deliverable 3.3**: Implement `estimate_nb` in `naive_bayes.py`. This function will estimate label-specific weights ($\\theta_{y,j}$ in the notation used by Eisenstein) in the same data format as the weights in `theta_hand` above. \n",
    "* **Test**: `tests/test_classifier.py:test_d3_3a_nb`\n",
    "\n",
    "* The goal is that the score given by `clf_base.predict` is equal to the joint probability $P(x,y)$ as described in Chapter 2 of the Eisenstein text.\n",
    "* Remember, the offset feature will act as the _prior_ $\\log P(y)$, and should not be smoothed\n",
    "\n",
    "_Tip_: Recall that each weight is supposed to represent the conditional probability of a particular word occurring in a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta_nb = naive_bayes.estimate_nb(x_train_pruned, y_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2000s',\n",
       " {'2000s': -2099.2474010561377,\n",
       "  'pre-1980': -2136.8348423968027,\n",
       "  '1980s': -2153.019927798135,\n",
       "  '1990s': -2125.1966084804503})"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_base.predict(x_train_pruned[155], theta_nb, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46444444444444444\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_base.predict_all(x_dev_pruned, theta_nb, labels)\n",
    "print(evaluation.acc(y_hat, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the submission of this assignment, you will provide me with your classifier's predictions, which I will validate as part of grading. There are utility functions in `hw2_utils.evaluation` that will automatically produce output in the required format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.write_predictions(y_hat, 'nb-dev.preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a function to read them back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46444444444444444"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_dev = evaluation.read_predictions('nb-dev.preds')\n",
    "evaluation.acc(y_hat_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to produce your predicted classifications on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = clf_base.predict_all(x_test_pruned, theta_nb, labels)\n",
    "evaluation.write_predictions(y_hat_test, 'nb-test.preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have set our smoothing parameter to 0.1. As we saw earlier, though, different amounts of smoothing can have big effects on our learned model weights.\n",
    "\n",
    "* **Deliverable 3.4**: Implement the `find_best_smoother` function in `naive_bayes.py`, which will grid search over a range of smoothing factors and find the optimal one.\n",
    "* **Test**: `tests/test_classifier.py:test_d3_4a_nb_best`\n",
    "\n",
    "Your function should try at least the values set in the `vals` variable below. Once you have obtained the optimal smoothing value, export your predictions on the test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-03 3.16227766e-03 1.00000000e-02 3.16227766e-02\n",
      " 1.00000000e-01 3.16227766e-01 1.00000000e+00 3.16227766e+00\n",
      " 1.00000000e+01 3.16227766e+01 1.00000000e+02]\n"
     ]
    }
   ],
   "source": [
    "vals = np.logspace(-3, 2, 11)\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(naive_bayes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_smoother, scores = naive_bayes.find_best_smoother(\n",
    "    x_train_pruned, y_train, x_dev_pruned, y_dev, vals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'dev set accuracy')"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(list(scores.keys()), list(scores.values()), 'o-');\n",
    "plt.xlabel('smoothing factor')\n",
    "plt.ylabel('dev set accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflect**:\n",
    "\n",
    "* What might explain the dramatic drop in accuracy when the smoothing increases from 10 to 30?\n",
    "* Beore you check, predict whether hte accuracy will continue to significantly drop if you increase it to 100000.\n",
    "\n",
    "End this section by writing your best predictions out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_nb_best = naive_bayes.estimate_nb(x_train_pruned, y_train, best_smoother)\n",
    "y_hat_nb_best = clf_base.predict_all(x_test_pruned, theta_nb_best, labels)\n",
    "evaluation.write_predictions(y_hat_nb_best, 'nb-best-test.preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "You can add non-BoW features to your model (in fact, we already have: our bias/offset feature). In order to facilitate additional exploration along these lines, we'll need to change how we're calling our `predict()` function. Up to now, we've been passing it a `Counter` object; now we need to turn that `Counter` into a dictionary, so we can add additional arbitrary features.\n",
    "\n",
    "## 4.1 Example feature: Token-Type Ratio\n",
    "\n",
    "One particularly useful additional feature is the token-type ratio:\n",
    "\n",
    "$$ \\frac{\\text{length of song in tokens}}{\\text{number of distinct types}} = \\frac{\\sum_m w_m}{\\sum_m \\delta(w_m > 0)} $$\n",
    "\n",
    "Intuititively, the more repetitive a song is, the higher its token-type ratio will be. For empty songs (length of zero tokens), this feature should be 0.\n",
    "\n",
    "* **Deliverable 7.1**: Implement `get_token_type_ratio` in `features.py`\n",
    "* **Test**: `tests/test_features.py:test_d4_1_token_type_ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2_utils import features\n",
    "reload(features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token-type ratio for the first five songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.083333333333333,\n",
       " 2.6,\n",
       " 1.9113924050632911,\n",
       " 2.318840579710145,\n",
       " 6.188679245283019]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[features.get_token_type_ratio(x_train_pruned[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a token-type ratio, we must include it in our feature vectors. A good way to do that is to _bin_ the feature into several bins:\n",
    "\n",
    "$\\{[(0,1),[1,2),[2,3),[3,4),[4,5),[5,6),[6,\\infty)\\}$\n",
    "\n",
    "In other words, for each instance, we will now have seven additional pseudo-features, one per bin. Exactly one will have the value of 1, the rest will be set to zero. Look in `hw2_utils.constants` for feature names for these bins.\n",
    "\n",
    "* **Deliverable 4.2**: Implement `concat_ttr_binned_feaures` in `features.py`\n",
    "* **Test**: `tests/test_features.py:test_d4_2_discretize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(features);\n",
    "reload(constants);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'back': 37,\n",
       " 'i': 34,\n",
       " 'im': 22,\n",
       " 'in': 22,\n",
       " 'love': 20,\n",
       " 'again': 15,\n",
       " 'right': 12,\n",
       " 'every': 10,\n",
       " 'time': 10,\n",
       " 'around': 7,\n",
       " 'turn': 6,\n",
       " 'seems': 6,\n",
       " 'like': 6,\n",
       " 'when': 5,\n",
       " 'me': 5,\n",
       " 'just': 5,\n",
       " 'alright': 5,\n",
       " 'move': 4,\n",
       " 'lose': 4,\n",
       " 'look': 4,\n",
       " 'and': 4,\n",
       " 'get': 3,\n",
       " 'mixed': 3,\n",
       " 'emotions': 3,\n",
       " 'its': 3,\n",
       " 'dont': 3,\n",
       " 'so': 3,\n",
       " 'to': 3,\n",
       " 'cant': 3,\n",
       " 'it': 3,\n",
       " 'help': 3,\n",
       " 'my': 2,\n",
       " 'know': 2,\n",
       " 'you': 2,\n",
       " 'your': 2,\n",
       " 'want': 2,\n",
       " 'believe': 2,\n",
       " 'many': 2,\n",
       " 'oh': 2,\n",
       " 'the': 1,\n",
       " 'way': 1,\n",
       " 'feelings': 1,\n",
       " 'flow': 1,\n",
       " 'excuse': 1,\n",
       " 'only': 1,\n",
       " 'human': 1,\n",
       " 'sometimes': 1,\n",
       " 'put': 1,\n",
       " 'arms': 1,\n",
       " 'feel': 1,\n",
       " 'satisfied': 1,\n",
       " 'forever': 1,\n",
       " 'decide': 1,\n",
       " 'now': 1,\n",
       " 'lover': 1,\n",
       " 'hard': 1,\n",
       " 'be': 1,\n",
       " 'a': 1,\n",
       " 'friend': 1,\n",
       " 'thats': 1,\n",
       " 'something': 1,\n",
       " 'else': 1,\n",
       " 'guess': 1,\n",
       " 'ever': 1,\n",
       " 'end': 1,\n",
       " 'that': 1,\n",
       " 'jones': 1,\n",
       " 'coming': 1,\n",
       " 'down': 1,\n",
       " 'yeah': 1,\n",
       " 'yes': 1,\n",
       " 'do': 1,\n",
       " '**TTR_0_1**': 0,\n",
       " '**TTR_1_2**': 0,\n",
       " '**TTR_2_3**': 0,\n",
       " '**TTR_3_4**': 0,\n",
       " '**TTR_4_5**': 1,\n",
       " '**TTR_5_6**': 0,\n",
       " '**TTR_6_INF**': 0}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.concat_ttr_binned_features(dict(x_dev_pruned[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Does it affect performance?\n",
    "\n",
    "If we use this new feature in our prediction, what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pruned_with_ttr = [features.concat_ttr_binned_features(dict(x_i)) \n",
    "                           for x_i in x_train_pruned]\n",
    "\n",
    "theta_nb_ttr = naive_bayes.estimate_nb(x_train_pruned_with_ttr, y_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4711111111111111\n"
     ]
    }
   ],
   "source": [
    "x_dev_pruned_with_ttr = [features.concat_ttr_binned_features(dict(x_i)) \n",
    "                           for x_i in x_dev_pruned]\n",
    "\n",
    "y_hat_ttr = clf_base.predict_all(x_dev_pruned_with_ttr, theta_nb_ttr, labels)\n",
    "print(evaluation.acc(y_hat_ttr, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like that feature got us a small performance bump!\n",
    "\n",
    "_Reflect_: What are some additional features that might help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression\n",
    "\n",
    "In practice, it is rare to build the entire feature-generation pipeline entirely from scratch as we have thus far in this assignment. Scikit-Learn  (`sklearn`) is a massive Python library with many useful off-the-shelf machine learning models, as well as utility functions for feature extraction and transformation. In this section, you will experiment with an off-the-shelf implementation of logistic regression (which we may revisit in more detail in a future assignment).\n",
    "\n",
    "A `sklearn` workflow typically starts by using one of the automated feature extraction classes. The equivalent to the simple count-based bag-of-words that we've been using thus far is [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer). There are several text feature extraction classes, and they have a variety of options in terms of tokenization, punctuation handling, etc.- consult the documentation!\n",
    "\n",
    "This part of the assignment will require much less coding, and is more exploratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already know what we want our vocabulary to be, we can initialize it with an iterable of strings; otherwise, it can build its vocabulary automatically (using its `fit()` method). In this case, we will initialize it to use our already-pruned vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x4875 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 377 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "count_vec.transform(df_train.Lyrics[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat_dev = count_vec.transform(df_dev.Lyrics)\n",
    "X_mat_train = count_vec.transform(df_train.Lyrics)\n",
    "X_mat_test = count_vec.transform(df_test.Lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data is in the `scipy` sparse matrix format, we can use any of the built-in `sklearn` classifiers. For example, naïve Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice thing about `sklearn` is that all of the classifiers follow a common API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_mat_train, df_train.Era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_dev = nb.predict(X_mat_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice thing: `sklearn` has many built-in utility functions for evaluation classifier performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_hat_dev, df_dev.Era)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the same accuracy as we saw with our hand-rolled implementation!\n",
    "\n",
    "We can also print confusion matrices, and more detailed classification output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54, 33, 28, 34],\n",
       "       [16, 25, 16, 22],\n",
       "       [13, 25, 68,  8],\n",
       "       [21, 12, 15, 60]])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_hat_dev, df_dev.Era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       1980s       0.52      0.36      0.43       149\n",
      "       1990s       0.26      0.32      0.29        79\n",
      "       2000s       0.54      0.60      0.56       114\n",
      "    pre-1980       0.48      0.56      0.52       108\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       450\n",
      "   macro avg       0.45      0.46      0.45       450\n",
      "weighted avg       0.47      0.46      0.46       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_hat_dev, df_dev.Era))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2_utils import clf_sklearn\n",
    "reload(clf_sklearn);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, explore the multinomial logistic regression classifier in `sklearn.linear_model.LogisticRegression`. Pay attention \n",
    "\n",
    "* **Deliverable 5.1**: Implement `train_logistic_regression` in `clf_sklearn.py`\n",
    "* **Test**: `tests/test_clf_sklearn.py:test_d5_1_train_logistic`\n",
    "\n",
    "What is the classification accuracy for this classifier using bag-of-words features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common to transform raw count features before using them as input into a classifier. One important transformation is _Term Frequency/Inverse Document Frequency_ (TF/IDF), which attempts to normalize for the fact that certain words have inflated raw counts because they are more commonly used across all documents (\"the\" is a good example).\n",
    "\n",
    "`sklearn` has a [`sklearn.feature_extraction.text.TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) class, which can transform a count matrix producd using the `CountVectorizer` class we used above. (There is also a `TfidfVectorizer` that can do the counting and transformation in one step).\n",
    "\n",
    "Adjust your model to use TF/IDF-adjusted word counts. Note that, just as we earlier had to \"pin\" our vocabulary to that used in the training set, we need to first _fit_ the transformer- in other words, tell it which document frequencies to use. You'll want to make sure those are consistent across train/dev/test splits.\n",
    "\n",
    "* **Deliverable 5.2**: Implement `transform_tf_idf` in `clf_sklearn.py`, and use it to train and evaluate a new logistic regression model uing the transformed counts.\n",
    "* **Test**: `tests/test_clf_sklearn.py:test_d5_2_tf_idf`\n",
    "\n",
    "_Reflect_: What happens to the classification accuracy of the logistic regression model using adjusted feature counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(vocabulary=vocab)\n",
    "X_train_counts = count_vec.transform(df_train.Lyrics)\n",
    "X_dev_counts = count_vec.transform(df_dev.Lyrics)\n",
    "X_test_counts = count_vec.transform(df_test.Lyrics)\n",
    "(train_tfidf, dev_tfidf, test_dfidf), tf_transformer = clf_sklearn.transform_tf_idf(X_train_counts, X_dev_counts, X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask a `Vectorizer` for an ordered list of feature labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'i', 'the', 'to', 'and']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.get_feature_names()[:5] # list just the first five features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_song = count_vec.transform([\"you and me and you\"])\n",
    "short_song.toarray() # note the value of feature 0 (\"you\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorter_song = count_vec.transform([\"you and me and\"])\n",
    "shorter_song.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6607231253680398\n",
      "0.4028249281653867\n"
     ]
    }
   ],
   "source": [
    "short_song_tfidf = tf_transformer.transform(short_song)\n",
    "shorter_song_tfidf = tf_transformer.transform(shorter_song)\n",
    "\n",
    "print(short_song_tfidf.toarray()[0,0])\n",
    "print(shorter_song_tfidf.toarray()[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End!\n",
    "\n",
    "Run the `make_submission.sh` script to package up your submission, and turn in your results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
